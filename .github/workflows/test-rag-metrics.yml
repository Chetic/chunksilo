name: Test RAG Metrics

on:
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to test (leave empty for current branch)'
        required: false
        type: string

permissions:
  contents: read

jobs:
  test-rag-metrics:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Check out repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch || github.ref }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt
          python -m pip install -r test/requirements.txt

      - name: Run large-scale test suite
        timeout-minutes: 20
        env:
          OFFLINE: 0
          TEST_DATA_DIR: ./test_data
          TEST_STORAGE_DIR: ./test_storage
          TEST_RESULTS_DIR: ./test_results
          ABORT_ON_DOWNLOAD_FAILURE: 1
        run: |
          cd test
          python test_large_scale.py

      - name: Check metrics against thresholds
        run: |
          # Find the most recent test results file
          RESULTS_FILE=$(ls -t test/test_results/test_results_*.json | head -n 1)
          
          if [ -z "$RESULTS_FILE" ]; then
            echo "‚ùå No test results file found"
            exit 1
          fi
          
          echo "üìä Checking metrics from: $RESULTS_FILE"
          
          # Extract metrics using Python (more reliable than jq for nested JSON)
          python3 << EOF
          import json
          import sys
          import glob
          
          # Thresholds - fail if metrics fall below these values
          THRESHOLDS = {
              "precision@5": 0.40,   # Current: 0.491
              "recall@5": 0.60,      # Current: 0.697
              "mrr": 0.45,            # Current: 0.518
              "ndcg@5": 0.60,         # Current: 0.672
          }
          
          # Find the most recent results file
          results_files = sorted(glob.glob("test/test_results/test_results_*.json"), reverse=True)
          if not results_files:
              print("‚ùå No test results file found")
              sys.exit(1)
          
          results_file = results_files[0]
          print(f"Using results file: {results_file}")
          
          with open(results_file, "r") as f:
              results = json.load(f)
          
          aggregate_metrics = results.get("aggregate_metrics", {})
          
          if not aggregate_metrics:
              print("‚ùå No aggregate metrics found in results")
              sys.exit(1)
          
          print("\n" + "=" * 60)
          print("RAG Metrics Validation")
          print("=" * 60)
          
          failed = False
          
          for metric_name, threshold in THRESHOLDS.items():
              actual_value = aggregate_metrics.get(metric_name, 0.0)
              
              if actual_value < threshold:
                  print(f"‚ùå {metric_name}: {actual_value:.3f} < {threshold:.3f} (FAILED)")
                  failed = True
              else:
                  print(f"‚úÖ {metric_name}: {actual_value:.3f} >= {threshold:.3f} (PASSED)")
          
          print("=" * 60)
          
          if failed:
              print("\n‚ùå One or more metrics fell below thresholds!")
              print("\nCurrent thresholds:")
              for metric, threshold in THRESHOLDS.items():
                  print(f"  {metric}: >= {threshold:.3f}")
              print("\nTo update thresholds, edit .github/workflows/test-rag-metrics.yml")
              sys.exit(1)
          else:
              print("\n‚úÖ All metrics passed thresholds!")
              sys.exit(0)
          EOF

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: rag-test-results
          path: test/test_results/*.json
          retention-days: 7

